name: Sentence Level Transcription

on:
  workflow_run:
    workflows: ["Transcript"]
    types:
      - completed
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - 'Visuals/**'
      - 'Trans/**'

permissions:
  contents: write

jobs:
  generate-transcription:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name != 'workflow_run' }}

    steps:  
      - name: Checkout repository  
        uses: actions/checkout@v3  

      - name: Set up Python  
        uses: actions/setup-python@v4  
        with:  
          python-version: '3.10'  

      - name: Generate sentence-level transcription  
        run: |  
          python3 - << 'PYTHON_SCRIPT_END'
          import json
          import os
          import re
          from datetime import datetime
          from difflib import SequenceMatcher

          def normalize(text):
              """Strips everything but lowercase letters and numbers."""
              if not text: return ""
              return re.sub(r"[^a-z0-9]", '', text.lower())

          def get_sim(a, b):
              return SequenceMatcher(None, a, b).ratio()

          def main():
              trans_p = "Trans/transcription.json"
              vis_p = "Visuals/visuals.json"
              
              if not os.path.exists(trans_p) or not os.path.exists(vis_p):
                  print(f"File missing: {trans_p} or {vis_p}")
                  return

              with open(trans_p, 'r') as f: trans_data = json.load(f)
              with open(vis_p, 'r') as f: visuals = json.load(f)

              # Flatten transcript into individual words
              words_db = []
              for seg in trans_data.get("segments", []):
                  for w in seg.get("words", []):
                      words_db.append({
                          "norm": normalize(w["word"]),
                          "start": w["start"],
                          "end": w["end"]
                      })

              results = []
              db_ptr = 0
              last_time = 0.0

              for item in visuals:
                  orig_sent = item.get("sentence", "")
                  target_words = [normalize(w) for w in orig_sent.split() if normalize(w)]
                  target_norm = "".join(target_words)
                  
                  # Handle visual-only/empty sentences - IMPROVED
                  if not target_norm: 
                      # Instead of fixed duration, use a smaller value to prevent long gaps
                      start_t = round(last_time + 0.01, 3)
                      end_t = round(start_t + 0.3, 3)  # REDUCED from 1.0s to 0.3s
                      results.append({
                          "id": f"i{item['number']}",
                          "number": item["number"],
                          "sentence": orig_sent,
                          "start": start_t,
                          "end": end_t,
                          "duration": 0.3
                      })
                      last_time = end_t
                      # DON'T advance pointer - next sentence should start from same position
                      continue

                  # === STRATEGY PART 1: IMMEDIATE NEIGHBOR CHECK - IMPROVED ===
                  immediate_match = None
                  immediate_score = 0.0
                  
                  min_len = max(1, len(target_words) - 3)  # More flexible
                  max_len = len(target_words) + 5  # Allow more variation
                  
                  if db_ptr < len(words_db):
                      for length in range(min_len, max_len):
                          end_i = db_ptr + length
                          if end_i > len(words_db): break
                          
                          candidate_norm = "".join([words_db[k]["norm"] for k in range(db_ptr, end_i)])
                          score = get_sim(target_norm, candidate_norm)
                          
                          if score > immediate_score:
                              immediate_score = score
                              immediate_match = (db_ptr, end_i)

                  # LOWERED threshold from 0.8 to 0.65 for better matching
                  if immediate_score > 0.65:
                      s_idx, e_idx = immediate_match
                      s_time = words_db[s_idx]["start"]
                      e_time = words_db[e_idx-1]["end"]
                      
                      # Ensure no overlap with previous segment
                      if s_time < last_time: 
                          s_time = last_time + 0.01
                      if e_time <= s_time: 
                          e_time = s_time + 0.5
                      
                      # Ensure minimum duration of 0.3s for readability
                      if (e_time - s_time) < 0.3:
                          e_time = s_time + 0.3

                      results.append({
                          "id": f"i{item['number']}",
                          "number": item["number"],
                          "sentence": orig_sent,
                          "start": round(s_time, 3),
                          "end": round(e_time, 3),
                          "duration": round(e_time - s_time, 3)
                      })
                      db_ptr = e_idx
                      last_time = e_time
                      continue

                  # === STRATEGY PART 2: WIDE SEARCH - IMPROVED ===
                  best_match_span = None
                  best_weighted_score = -1.0
                  search_limit = min(db_ptr + 3000, len(words_db))  # Extended search range
                  base_threshold = 0.55 if len(target_words) < 5 else 0.5  # LOWERED thresholds
                  dist_penalty = 0.0005  # REDUCED penalty from 0.001 to 0.0005

                  for start_i in range(db_ptr, search_limit):
                      distance = start_i - db_ptr
                      penalty = distance * dist_penalty
                      if (1.0 - penalty) < best_weighted_score: break

                      for length in range(min_len, max_len):
                          end_i = start_i + length
                          if end_i > len(words_db): break
                          
                          candidate_norm = "".join([words_db[k]["norm"] for k in range(start_i, end_i)])
                          raw_score = get_sim(target_norm, candidate_norm)
                          
                          weighted_score = raw_score - penalty
                          if weighted_score > best_weighted_score:
                              best_weighted_score = weighted_score
                              best_match_span = (start_i, end_i)

                  final_raw_score = 0
                  if best_match_span:
                      s, e = best_match_span
                      candidate_norm = "".join([words_db[k]["norm"] for k in range(s, e)])
                      final_raw_score = get_sim(target_norm, candidate_norm)

                  if final_raw_score > base_threshold and best_match_span:
                      s_idx, e_idx = best_match_span
                      s_time = words_db[s_idx]["start"]
                      e_time = words_db[e_idx-1]["end"]
                      
                      if s_time < last_time: 
                          s_time = last_time + 0.01
                      if e_time <= s_time: 
                          e_time = s_time + 0.5
                      
                      # Ensure minimum duration
                      if (e_time - s_time) < 0.3:
                          e_time = s_time + 0.3

                      results.append({
                          "id": f"i{item['number']}",
                          "number": item["number"],
                          "sentence": orig_sent,
                          "start": round(s_time, 3),
                          "end": round(e_time, 3),
                          "duration": round(e_time - s_time, 3)
                      })
                      db_ptr = e_idx
                      last_time = e_time
                  else:
                      # Fallback: Visual Gap / Not Found - IMPROVED
                      s_time = round(last_time + 0.01, 3)
                      e_time = round(s_time + 0.5, 3)  # REDUCED from 1.5s to 0.5s
                      results.append({
                          "id": f"i{item['number']}",
                          "number": item["number"],
                          "sentence": orig_sent,
                          "start": s_time,
                          "end": e_time,
                          "duration": 0.5
                      })
                      last_time = e_time
                      # DON'T advance db_ptr - let next sentence try from same position

              output = {
                  "metadata": {
                      "created_at": datetime.utcnow().isoformat() + "Z",
                      "total": len(results)
                  },
                  "sentence_transcriptions": results
              }

              os.makedirs("Edits", exist_ok=True)
              with open("Edits/edit.json", "w") as f:
                  json.dump(output, f, indent=2)
              print(f"âœ… Processed {len(results)} sentences with improved sync.")

          if __name__ == "__main__":
              main()
          PYTHON_SCRIPT_END

      - name: Configure Git  
        run: |  
          git config --global user.name "intellectra9"  
          git config --global user.email "intellectra9@outlook.com"  

      - name: Commit and push transcription file  
        env:  
          GH_PAT: ${{ secrets.GH_PAT }}  
        run: |  
          git stash --include-untracked  
          git pull origin main --rebase || echo "No rebase needed"  
          git stash pop || true  
          git add Edits/edit.json  
          timestamp=$(TZ="Asia/Kolkata" date +"%Y-%m-%d %H:%M:%S IST")  
          git commit -m "ðŸŽ¯ Improved Sync (Fixed Thresholds): ${timestamp}" || echo "No changes"  
          git push https://x-access-token:${GH_PAT}@github.com/${{ github.repository }}.git HEAD:main

