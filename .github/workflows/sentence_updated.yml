name: Sentence Level Transcription

on:
  workflow_run:
    workflows: ["Transcript"]
    types:
      - completed
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - 'Visuals/**'
      - 'Trans/**'

permissions:
  contents: write

jobs:
  generate-transcription:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name != 'workflow_run' }}

    steps:  
      - name: Checkout repository  
        uses: actions/checkout@v3  

      - name: Set up Python  
        uses: actions/setup-python@v4  
        with:  
          python-version: '3.10'  

      - name: Generate sentence-level transcription  
        run: |  
          python3 << 'EOF'  
          import json  
          import os  
          import re  
          from datetime import datetime  
          from difflib import SequenceMatcher  

          def clean_text(text):  
              """Universal text cleaning: lowercase and alphanumeric only."""  
              if not text: return ""
              return re.sub(r"[^a-z0-9\s]", '', text.lower()).strip()

          def get_similarity(a, b):  
              """Calculate similarity ratio between two normalized strings."""  
              return SequenceMatcher(None, a, b).ratio()  

          def main():  
              trans_path = "Trans/transcription.json"  
              visuals_path = "Visuals/visuals.json"  
                
              if not os.path.exists(trans_path) or not os.path.exists(visuals_path):  
                  print("Error: Missing source files at " + trans_path + " or " + visuals_path)  
                  return  

              with open(trans_path, 'r', encoding='utf-8') as f:  
                  trans_data = json.load(f)  
              with open(visuals_path, 'r', encoding='utf-8') as f:  
                  visuals = json.load(f)  

              # Extract all words with their metadata into a flat list
              all_words = []  
              for seg in trans_data.get("segments", []):  
                  if "words" in seg:  
                      for w in seg["words"]:  
                          all_words.append({  
                              "clean": clean_text(w["word"]),  
                              "start": w["start"],  
                              "end": w["end"]  
                          })  

              results = []  
              current_ptr = 0  
              last_end_time = 0.0  

              for item in visuals:  
                  sentence = item.get("sentence", "")  
                  target = clean_text(sentence)  
                  target_word_count = len(target.split())
                  
                  if not target: continue  

                  best_match = None  
                  highest_score = 0  
                  
                  # Universal Sliding Window: 
                  # We search from the current pointer up to a reasonable 'lookahead' (30 words)
                  # to find the window of words that most closely resembles the target sentence.
                  search_limit = min(current_ptr + 30, len(all_words))
                  
                  for start_i in range(current_ptr, search_limit):
                      # We check spans of words similar in length to our target sentence
                      # (+/- 5 words to account for transcription differences)
                      for length in range(max(1, target_word_count - 5), target_word_count + 8):
                          end_i = start_i + length
                          if end_i > len(all_words): break
                          
                          candidate_segment = " ".join([all_words[k]["clean"] for k in range(start_i, end_i)])
                          score = get_similarity(target, candidate_segment)
                          
                          if score > highest_score:
                              highest_score = score
                              best_match = (start_i, end_i)

                  # If match is decent (> 0.5 similarity), use the timestamps
                  if highest_score > 0.5 and best_match:
                      s_idx, e_idx = best_match
                      start_val = all_words[s_idx]["start"]
                      end_val = all_words[e_idx-1]["end"]
                      
                      results.append({
                          "id": f"i{item['number']}",
                          "number": item["number"],
                          "sentence": sentence,
                          "start": round(start_val, 3),
                          "end": round(end_val, 3),
                          "duration": round(end_val - start_val, 3)
                      })
                      # Move the pointer to the end of this match to maintain sequential order
                      current_ptr = e_idx
                      last_end_time = end_val
                  else:
                      # Fallback logic for visual-only items or failed matches
                      duration = 1.5 # Default fallback
                      start_val = round(last_end_time + 0.05, 3)
                      end_val = round(start_val + duration, 3)
                      results.append({
                          "id": f"i{item['number']}",
                          "number": item["number"],
                          "sentence": sentence,
                          "start": start_val,
                          "end": end_val,
                          "duration": duration
                      })
                      last_end_time = end_val

              output = {  
                  "metadata": {  
                      "created_at": datetime.utcnow().isoformat() + "Z",  
                      "total_sentences": len(results),  
                      "source_files": {"visuals": visuals_path, "transcription": trans_path}  
                  },  
                  "sentence_transcriptions": results  
              }  

              os.makedirs("Edits", exist_ok=True)  
              with open("Edits/edit.json", "w", encoding="utf-8") as f:  
                  json.dump(output, f, indent=2, ensure_ascii=False)  
                
              print(f"âœ… Success: Processed {len(results)} sentences.")  

          if __name__ == "__main__":  
              main()  
          EOF  

      - name: Configure Git  
        run: |  
          git config --global user.name "intellectra9"  
          git config --global user.email "intellectra9@outlook.com"  

      - name: Commit and push transcription file  
        env:  
          GH_PAT: ${{ secrets.GH_PAT }}  
        run: |  
          git stash --include-untracked  
          git pull origin main --rebase || echo "No rebase needed"  
          git stash pop || true  
          git add Edits/edit.json  
          timestamp=$(TZ="Asia/Kolkata" date +"%Y-%m-%d %H:%M:%S IST")  
          git commit -m "ðŸ”„ Universal Word-Level Sync: ${timestamp}" || echo "No changes"  
          git push https://x-access-token:${GH_PAT}@github.com/${{ github.repository }}.git HEAD:main


