name: Transcript

on:
  workflow_run:
    workflows: ["Voiceover"]  # Must match the workflow name exactly
    types:
      - completed
  workflow_dispatch:  # Manual trigger

jobs:
  transcribe:
    runs-on: ubuntu-latest

    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name != 'workflow_run' }}

    permissions:
      contents: write
      actions: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Pull latest changes
        run: |
          git config --local user.name "intellectra9"
          git config --local user.email "intellectra9@outlook.com"
          git pull origin main --rebase || git pull origin main

      - name: Clean preinstalled packages to free space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf /usr/local/share/boost
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          echo "ðŸ§¹ Space freed"

      - name: Install ffmpeg
        run: sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install dependencies (lightweight)
        run: |
          python -m pip install --upgrade pip
          pip install torch==2.4.1+cpu -f https://download.pytorch.org/whl/torch_stable.html
          pip install git+https://github.com/openai/whisper.git --no-deps
          pip install git+https://github.com/m-bain/whisperx.git --no-deps
          pip install numpy soundfile tqdm

      - name: Transcribe entire audio with WhisperX word-level timestamps
        run: |
          python - << 'EOF'
          import whisperx
          import os
          import json

          device = "cpu"
          compute_type = "float32"
          batch_size = 16
          audio_file = "Audio/a1.mp3"

          print("Loading WhisperX model...")
          model = whisperx.load_model(
              "base",
              device,
              compute_type=compute_type,
              language="en",
              vad_filter=False  # FIX: prevents Pyannote VAD crash
          )

          print("Loading audio...")
          audio = whisperx.load_audio(audio_file, sr=16000)

          print("Transcribing audio...")
          result = model.transcribe(audio, batch_size=batch_size)
          print("Initial transcription completed")

          print("Loading alignment model...")
          model_a, metadata = whisperx.load_align_model(
              language_code=result["language"],
              device=device
          )

          print("Aligning for word-level timestamps...")
          result = whisperx.align(
              result["segments"],
              model_a,
              metadata,
              audio,
              device,
              return_char_alignments=False
          )
          print("Word-level alignment completed")

          os.makedirs("Trans", exist_ok=True)
          files_to_replace = ["Trans/t1.txt", "Trans/timestamps.txt", "Trans/transcription.json"]

          for file_path in files_to_replace:
              if os.path.exists(file_path):
                  os.remove(file_path)

          with open("Trans/t1.txt", "w", encoding="utf-8") as f:
              f.write("WhisperX FULL Word-Level Transcription Results\n")
              f.write("=" * 60 + "\n\n")
              for i, segment in enumerate(result["segments"]):
                  text = segment.get("text", "").strip()
                  start = segment.get("start", 0)
                  end = segment.get("end", 0)
                  f.write(f"SEGMENT {i+1}: [{start:.2f}s - {end:.2f}s]\n")
                  f.write(f"Text: \"{text}\"\n")
                  if "words" in segment and segment["words"]:
                      f.write("Word-level timestamps:\n")
                      for j, word in enumerate(segment["words"]):
                          wtext = word.get("word", "").strip()
                          wstart = word.get("start", 0)
                          wend = word.get("end", 0)
                          f.write(f"  {j+1:2d}. \"{wtext}\" -> {wstart:.2f}s - {wend:.2f}s\n")
                  f.write("\n" + "-" * 50 + "\n\n")

          with open("Trans/timestamps.txt", "w", encoding="utf-8") as f:
              for segment in result["segments"]:
                  if "words" in segment:
                      for word in segment["words"]:
                          f.write(f"{word['word']},{word['start']:.2f},{word['end']:.2f}\n")

          with open("Trans/transcription.json", "w", encoding="utf-8") as f:
              json.dump(result, f, indent=2, ensure_ascii=False)

          print("âœ… WhisperX transcription completed!")
          EOF

      - name: Commit and push transcription files
        run: |
          git add Trans/
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            timestamp=$(TZ="Asia/Kolkata" date +"%Y-%m-%d %H:%M:%S IST")
            git commit -m "ðŸ”„ Update WhisperX transcription: ${timestamp}"
            git pull origin main --rebase || git pull origin main --strategy-option=ours
            git push origin main
            echo "ðŸ“Œ Successfully pushed transcription files"
