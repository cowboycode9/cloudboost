name: WhisperX Transcription

on:
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Free disk space
      run: |
        sudo rm -rf /usr/share/dotnet
        sudo rm -rf /usr/local/lib/android
        sudo rm -rf /opt/ghc
        sudo rm -rf /usr/local/lib/node_modules
        sudo rm -rf /usr/local/share/powershell
        sudo rm -rf /usr/local/share/chromium

    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:
        python-version: "3.10"

    - name: Install PyTorch CPU
      run: |
        pip install --upgrade pip
        pip install torch==2.1.1+cpu torchvision==0.16.1+cpu torchaudio==2.1.1+cpu \
          --index-url https://download.pytorch.org/whl/cpu

    - name: Install WhisperX & dependencies
      run: |
        pip install git+https://github.com/m-bain/whisperx.git --no-deps
        pip install transformers==4.36.2
        pip install faster-whisper==0.10.0
        pip install ffmpeg-python soundfile numpy

    - name: Create and run transcription script
      run: |
        cat << 'EOF' > script.py
import whisperx, torch, json, os

AUDIO_FILE = "audio.mp3"
device = "cpu"
compute_type = "int8"

model = whisperx.load_model("large-v2", device, compute_type=compute_type)
audio = whisperx.load_audio(AUDIO_FILE)
result = model.transcribe(audio)

align_model, metadata = whisperx.load_align_model(
    language_code=result["language"], device=device
)
aligned_result = whisperx.align(
    result["segments"], align_model, metadata, audio, device
)

diarize_model = whisperx.DiarizationPipeline(use_auth_token=None, vad_filter=False)
diarize_segments = diarize_model(audio)
result = whisperx.assign_word_speakers(diarize_segments, aligned_result)

os.makedirs("output", exist_ok=True)

# JSON
with open("output/transcription.json", "w", encoding="utf-8") as f:
    json.dump(result, f, indent=2, ensure_ascii=False)

# TXT
with open("output/transcription.txt", "w", encoding="utf-8") as f:
    for seg in result["segments"]:
        speaker = seg.get("speaker", "Unknown")
        f.write(f"{speaker}: {seg['text'].strip()}\n")

# SRT
def format_srt_time(t):
    hours = int(t // 3600)
    minutes = int((t % 3600) // 60)
    seconds = t % 60
    return f"{hours:02}:{minutes:02}:{seconds:06.3f}".replace('.', ',')

with open("output/transcription.srt", "w", encoding="utf-8") as f:
    for i, seg in enumerate(result["segments"], start=1):
        start, end = seg["start"], seg["end"]
        speaker = seg.get("speaker", "Unknown")
        text = seg["text"].strip()
        f.write(f"{i}\n{format_srt_time(start)} --> {format_srt_time(end)}\n{speaker}: {text}\n\n")

print("ðŸŽ‰ Transcription complete! Files saved inside /output folder")
EOF
        python3 script.py

    - name: Upload transcription output
      uses: actions/upload-artifact@v3
      with:
        name: transcription_output
        path: output
